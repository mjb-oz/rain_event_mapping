{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasselled Cap Wetness Transform to look at extent of 'wetness' in the Southern Stuart Corridor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "This notebook allows the calculation of Tasselled Cap Wetness (TCW) for a given time series of landsat Scenes.\n",
    "The TCW is based on the Crist 1985 RF coefficients. Improved coeffieicents for the Australian Context are being developed by Dale Roberts, so the TCW function may need an update soon.\n",
    "\n",
    "\n",
    "NBART should be used for this analysis, as terrain shadow on southwest facing slopes can show up as 'wetness'.\n",
    "- would use nbart but having some terrain masking issues today...\n",
    "\n",
    "\n",
    "This is running it for all sensors - despite being the 1985 paper\n",
    "\n",
    "-------------------------------------------------------------------- \n",
    "The following sensors are available for the following time frames:\n",
    "* Landsat 5 - 1986 to April 1999  followed by a gap until May 2003 - November 2011 (data from 2009 onwards becomes less reliable in southern Australia)\n",
    "* Landsat 7 - April 1999 to present, however after May 2003 the scan line corrector (SLC) failed, \n",
    "so data are referred to as SLC-off, meaning they've got a venetian blinds appearance with wedges of missing data\n",
    "  * This data is not well suited for inclusion in composites, but is fine to use in time series analysis\n",
    "* Landsat 8 - April 2013 onwards\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-08T23:11:39.124969Z",
     "start_time": "2018-02-08T23:11:39.119569Z"
    }
   },
   "source": [
    "## import some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T00:41:33.824325Z",
     "start_time": "2018-02-09T00:41:33.765460Z"
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datacube\n",
    "from datacube.storage import masking\n",
    "from datacube.utils import geometry\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage.masking import mask_invalid_data\n",
    "from datacube.storage.storage import write_dataset_to_netcdf\n",
    "\n",
    "import gc\n",
    "\n",
    "import shapely\n",
    "from shapely.geometry import shape\n",
    "\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import pickle\n",
    "\n",
    "from skimage import exposure\n",
    "\n",
    "#suppress warnings thrown when using inequalities in numpy (the threshold values!)\n",
    "import warnings\n",
    "\n",
    "#import module to work with rgb images\n",
    "# from PIL import Image\n",
    "\n",
    "# def eprint(*args, **kwargs):\n",
    "#     print(*args, file=sys.stderr, **kwargs)\n",
    "        \n",
    "#libraries for polygon and polygon mask\n",
    "# import fiona\n",
    "# import shapely.geometry\n",
    "# import rasterio.features\n",
    "# import rasterio    \n",
    "# import json\n",
    "\n",
    "# import os.path\n",
    "# import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading & Preparation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: load_nbart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T00:03:47.129736Z",
     "start_time": "2018-02-09T00:03:47.055010Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_nbart(product,query,bands_of_interest): \n",
    "    '''\n",
    "    Loads NBART data and pixel quality as an Xarray\n",
    "    '''  \n",
    "    print('Loading ' + product)\n",
    "    \n",
    "    # define name of PQ product\n",
    "    pq_name = product.replace('nbart','pq')\n",
    "    \n",
    "    # make the datacube\n",
    "    dc = datacube.Datacube(app='tcw-ssc')\n",
    "    # load the data from datacube\n",
    "    ds = dc.load(product=product, measurements=bands_of_interest,\n",
    "                 group_by='solar_day', **query)\n",
    "    \n",
    "    # if the load returned anything grab the crs and affine\n",
    "    if ds:\n",
    "        crs = ds.crs\n",
    "        affine = ds.affine\n",
    "        # load the pixel quality layer for the same data\n",
    "        sensor_pq = dc.load(product= pq_name, fuse_func=ga_pq_fuser,\n",
    "                            group_by='solar_day', **query)\n",
    "    # combine the spectral and PQ data together\n",
    "    ds = xr.auto_combine([sensor_pq,ds])\n",
    "    # attach the attributs to the resultant dataset\n",
    "    ds.attrs['crs'] = crs\n",
    "    ds.attrs['affine'] = affine\n",
    "    print('Loaded ' + product)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: getData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T00:14:05.996584Z",
     "start_time": "2018-02-09T00:14:05.993058Z"
    }
   },
   "outputs": [],
   "source": [
    "def getData(study_area, flood_date, days_span, before_flood):\n",
    "    \"\"\"\n",
    "    This is a wrapper for the load_nbart function.\n",
    "    It allows different study areas to be referenced by name, and also does some case handling for\n",
    "    deciding which time series to load (either before or after the flood)\n",
    "    \"\"\"\n",
    "    # define the epoch based on the flood date, requested number of days, and before or after the flood\n",
    "    if before_flood:\n",
    "        end_of_epoch = datetime.date(int(flood_date[0:4]), int(flood_date[5:7]), int(flood_date[8::]))\n",
    "        start_of_epoch = end_of_epoch - datetime.timedelta(days = days_span)\n",
    "    else:\n",
    "        start_of_epoch = datetime.date(int(flood_date[0:4]), int(flood_date[5:7]), int(flood_date[8::]))\n",
    "        end_of_epoch = start_of_epoch + datetime.timedelta(days = days_span)\n",
    "        \n",
    "    # define the product of interest\n",
    "    if start_of_epoch > datetime.date(2013,4,1):\n",
    "        product =  'ls8_nbart_albers' \n",
    "    else:\n",
    "        product = 'ls7_nbart_albers'\n",
    "               \n",
    "    # define the bounding box\n",
    "    if study_area == 'tennantcreek':\n",
    "        lat_max = -19.18\n",
    "        lat_min = -20.35\n",
    "        lon_max = 134.34\n",
    "        lon_min = 133.41 \n",
    "    if study_area == 'westerndavenport1':\n",
    "        lat_max = -20.32\n",
    "        lat_min = -20.9\n",
    "        lon_max = 135.02\n",
    "        lon_min = 133.41   \n",
    "    if study_area == 'westerndavenport2':\n",
    "        lat_max = -20.9\n",
    "        lat_min = -21.5\n",
    "        lon_max = 135.02\n",
    "        lon_min = 133.41   \n",
    "    if study_area == 'westerndavenport3':\n",
    "        lat_max = -21.5\n",
    "        lat_min = -22.13\n",
    "        lon_max = 135.02\n",
    "        lon_min = 133.41  \n",
    "    if study_area == 'titree1':\n",
    "        lat_max = -21.64\n",
    "        lat_min = -22.4\n",
    "        lon_max = 134.31\n",
    "        lon_min = 132.94\n",
    "    if study_area == 'titree2':\n",
    "        lat_max = -22.4\n",
    "        lat_min = -23.13\n",
    "        lon_max = 134.31\n",
    "        lon_min = 132.94\n",
    "    if study_area == 'alicesprings':\n",
    "        lat_max = -23.42\n",
    "        lat_min = -24.06\n",
    "        lon_max = 134.44\n",
    "        lon_min = 133.51\n",
    "\n",
    "    # assemble the query \n",
    "    query = {'time': (start_of_epoch, end_of_epoch)}    #'dask_chunks': {'time': 5}\n",
    "    query['long'] = (lon_min, lon_max)\n",
    "    query['lat'] = (lat_min, lat_max)\n",
    "    query['crs'] = 'EPSG:4326'\n",
    "\n",
    "    # Define wavelengths/bands of interest, remove this kwarg to retrieve all bands\n",
    "    bands_of_interest = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']\n",
    "    \n",
    "    # actually load the data based on the query assembled\n",
    "    ds = load_nbart(product,query,bands_of_interest)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: maskCloudAndTerrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskCloudAndTerrain(ds, cloud = True, terrain = True):\n",
    "    \"\"\"\n",
    "    Use the pixel quality layer and the -999 for terrain shadowed pixels to np.NaN\n",
    "    \"\"\"\n",
    "    # 16383 is the value for a good (ie clear) pixel\n",
    "    if cloud:\n",
    "        ds = ds.where(ds.pixelquality == 16383)\n",
    "    # -999 occurs when either no data, or when pixel is masked by terrain correction\n",
    "    if terrain:\n",
    "        ds = ds.where(ds != -999)\n",
    "    # return the ds, change to float32 from default float64\n",
    "    return ds.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wetness Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: calc_wetness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T00:03:47.203583Z",
     "start_time": "2018-02-09T00:03:47.131769Z"
    }
   },
   "outputs": [],
   "source": [
    "#calculating tasselled cap wetness\n",
    "def calc_wetness(sensor_data,sensor):\n",
    "    '''\n",
    "    This function performs the tasselled cap transformation, multiplying band data by tasselled cap coefficients to \n",
    "    produce a \"wetness\" \"band\".\n",
    "    sensor_data is surface reflectance data loaded from the datacube\n",
    "    sensor = 'ls5, 'ls7' or 'ls8'\n",
    "    Coefficients are from Crist and Cicone 1985 \"A TM Tasseled Cap equivalent transformation for reflectance\n",
    "    factor data\" function written 23-08-2017 based on dc v1.5.1. updated 19-10-2017 bd\n",
    "    \n",
    "    Function supplied by Bex Dunn.\n",
    "    '''\n",
    "\n",
    "    wetness_coeff = {'ls5':{'blue':0.0315, 'green':0.2021, 'red':0.3102,\n",
    "                            'nir':0.1594, 'swir1':-0.6806, 'swir2':-0.6109},\n",
    "               ;;;;     'ls7':{'blue':0.0315, 'green':0.2021, 'red':0.3102,\n",
    "                           'nir':0.1594, 'swir1':-0.6806, 'swir2':-0.6109},\n",
    "                    'ls8':{'blue':0.0315, 'green':0.2021, 'red':0.3102,\n",
    "                           'nir':0.1594, 'swir1':-0.6806, 'swir2':-0.6109}}\n",
    "    #if there is sensor data for the time period\n",
    "    if sensor_data is not None: \n",
    "         # make a deep copy of the sensor data\n",
    "        wbg = sensor_data.copy(deep=True)\n",
    "        #iterate over the spectral bands\n",
    "        for band_name in sensor_data.data_vars:\n",
    "            #multiply each band by the transform coefficient to get a band-specific value\n",
    "            wetness_band = sensor_data[band_name]*wetness_coeff[sensor][band_name]\n",
    "            #update the existing band data with the TC data\n",
    "            #by making new bands, called wet_green, bright_green etc.\n",
    "            wbg.update({'wet_'+band_name:(['time','y','x'],wetness_band)})\n",
    "            #then drop the original bands\n",
    "            wbg = wbg.drop({band_name})    \n",
    "        #sum the values for each band to get the tcw dim    \n",
    "        wbg['wetness']=wbg.wet_blue+wbg.wet_green+wbg.wet_red+wbg.wet_nir+wbg.wet_swir1+wbg.wet_swir2\n",
    "        bands_to_drop =[]\n",
    "        for new_band in wbg.data_vars:\n",
    "            bands_to_drop.append(new_band)            \n",
    "        bands_to_drop.remove('wetness')    \n",
    "        wbg = wbg.drop(bands_to_drop)\n",
    "        print('calculated wetness for {}'.format(sensor))\n",
    "        return wbg    \n",
    "    else:\n",
    "        print('did not calculate wetness for {}'.format(sensor))\n",
    "        return None            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: calc_wetveg_overthresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T00:03:47.227410Z",
     "start_time": "2018-02-09T00:03:47.205504Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_wetveg_overthresh(wetness,threshold=-400):\n",
    "    '''\n",
    "    Calculate the wetness values where wetness>threshold. Inputs are wetness array and threshold value, \n",
    "    default threshold is -400. Band for wetness>threshold is added to wetness. This is not the count.\n",
    "    \n",
    "    Function supplied by Bex Dunn\n",
    "    '''\n",
    "    if wetness is not None:\n",
    "        with warnings.catch_warnings():\n",
    "            #suppress irritating behaviour in xarray.where\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            #water_plus_wetveg is wetness values where wetness>threshold\n",
    "            wetness['water_plus_wetveg'] = wetness.wetness.where(wetness.wetness>threshold)\n",
    "            print('thresholded wetness added to array')\n",
    "            return wetness\n",
    "    else:\n",
    "        print('did not calculate wetness overthreshold' )\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: counts_wets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T00:03:47.253515Z",
     "start_time": "2018-02-09T00:03:47.229926Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_wets(wetness):\n",
    "    '''\n",
    "    count the number of wetness scenes for each pixel,\n",
    "    count the amount of times that water plus wet veg is above the threshold\n",
    "    load both into memory (this assumes you are using dask),\n",
    "    return a dictionary of wet count and threshold count\n",
    "    \n",
    "    Function supplied by Bex Dunn\n",
    "    '''\n",
    "    if wetness is not None:\n",
    "        #count the number of wetness scenes for each pixel\n",
    "        wet_count = wetness.wetness.count(dim='time')\n",
    "\n",
    "        #count the amount of times that water plus wet veg is above the threshold\n",
    "        threshold_count= wetness.water_plus_wetveg.count(dim='time')\n",
    "        \n",
    "        #bring both counts into memory\n",
    "        wet_count.load()\n",
    "        threshold_count.load()\n",
    "        \n",
    "        #define dictionary of wet count and threshold count\n",
    "        counts = {'wet count':wet_count, 'threshold count':threshold_count}\n",
    "        print('counted')\n",
    "        return counts\n",
    "    else:\n",
    "        print('did not count' )\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: calcWetnessProportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcWetnessProportion(ds):\n",
    "    \"\"\"\n",
    "    This is a wrapper function for all the functions supplied by Bex.\n",
    "    \"\"\"\n",
    "    \n",
    "    # copy the attribs to preserve them for the end\n",
    "    attribs = ds.attrs\n",
    "    # calculate the TCW\n",
    "    wetness_sensor3_nbart = calc_wetness(ds,'ls8')\n",
    "    # find when wetness is over the threshold\n",
    "    water_plus_wetveg_3 = calc_wetveg_overthresh(wetness_sensor3_nbart)\n",
    "    # count number of time wetness is over threshold\n",
    "    counts_sensor_3_nbart = count_wets(wetness_sensor3_nbart)\n",
    "    # calculate proportion of time over threshold\n",
    "    wet_proportion_allsensors = counts_sensor_3_nbart['threshold count']/counts_sensor_3_nbart['wet count']\n",
    "    # reassign attribs to the result\n",
    "    wet_proportion_allsensors.attrs = attribs\n",
    "    # print a quick and dirty view of the result for simple visual check\n",
    "    imshow(wet_proportion_allsensors.values)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    # return the result\n",
    "    return wet_proportion_allsensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing and Output Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: write_your_netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T00:03:47.904592Z",
     "start_time": "2018-02-09T00:03:47.893288Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_your_netcdf(data, dataset_name, filename, crs):\n",
    "    '''\n",
    "    this function turns an xarray dataarray into a dataset so we can write it to netcdf. It adds on a crs definition\n",
    "    from the original array. data = your xarray dataset, dataset_name is a string describing your variable\n",
    "    \n",
    "    Function supplied by Bex Dunn\n",
    "    '''    \n",
    "    #turn array into dataset so we can write the netcdf\n",
    "    dataset= data.to_dataset(name=dataset_name)\n",
    "    #grab our crs attributes to write a spatially-referenced netcdf\n",
    "    dataset.attrs['crs'] = crs\n",
    "    #dataset.dataset_name.attrs['crs'] = crs\n",
    "    try:\n",
    "        write_dataset_to_netcdf(dataset, filename)\n",
    "    except RuntimeError as err:\n",
    "        print(\"RuntimeError: {0}\".format(err))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: drawTrueColour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawTrueColour(ds, time = 0):\n",
    "    \"\"\"\n",
    "    A quick function to view the scene as a true colour image.\n",
    "    Can change the time parameter to look at different satellite passes\n",
    "    \"\"\"\n",
    "    # store the shape of the data as variables\n",
    "    _, y, x = ds['red'].shape\n",
    "    # define an empty array of the correct size to store the RGB bands as float32s\n",
    "    rawimg = np.zeros((y,x,3), dtype = np.float32)\n",
    "    # loop through RBG, extract the data, and save it in the correct layer of the new array\n",
    "    for i, colour in enumerate(['red','green','blue']):\n",
    "        rawimg[:,:,i] = ds[colour][time].values\n",
    "    # remove the -999 values and convert to NaN\n",
    "    rawimg[rawimg == -999] = np.nan\n",
    "    # do a contrast stretch to make the image render in true colour for human interpretation\n",
    "    img_toshow = exposure.equalize_hist(rawimg, mask = np.isfinite(rawimg))\n",
    "    # make a figure object to draw onto\n",
    "    fig = plt.figure(figsize=[10,10])\n",
    "    # draw onto it\n",
    "    imshow(img_toshow)\n",
    "    # fetch the axes object to apply a title\n",
    "    ax = plt.gca()\n",
    "    # add a title\n",
    "    ax.set_title(str(ds.time[time].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: createGeoTiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGeoTiff(ds, study_area, flood_year):\n",
    "    \"\"\"\n",
    "    A function that saves the processed TCW data as a netCDF and as a geotif.\n",
    "    This is heavily based on code from Bex Dunn\n",
    "    \"\"\"\n",
    "    # construct the output filenames\n",
    "    filename = study_area + '_' + str(flood_year)\n",
    "    ncfile = filename + '.nc'\n",
    "    tiffile = filename + '.tif'\n",
    "\n",
    "    # save results as netCDF\n",
    "    write_your_netcdf(ds,'tcw',filename = ncfile, crs=ds.attrs['crs'])\n",
    "    print('Successfully wrote Tasselled Cap Wetness netCDF for ' + filename)\n",
    "\n",
    "    # convert netCDF to GeoTiff using the GDAL tool on the command line (ie not python!)\n",
    "    !gdalwarp -of GTiff $ncfile $tiffile\n",
    "    print('GeoTiff has been saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mosaic(ds, before_flood):\n",
    "    \"\"\"\n",
    "    This function looks forwards or backwards through a given time series taking the the most\n",
    "    recent valid pixel values (when looking backwards), or the earliest valid pixel values when looking forwards\n",
    "    \"\"\"\n",
    "    # define satellite bands to perform the aggregation for\n",
    "    sat_bands = ['blue','green','red','nir','swir1','swir2']\n",
    "    # define the length of the time series\n",
    "    l = len(ds['red'])\n",
    "    # create a dict for adding the resulting dataarrays to, for eventual reconstruction as a dataset\n",
    "    allres = {}\n",
    "    \n",
    "    for band in sat_bands:\n",
    "        if not before_flood: # ie if after flood\n",
    "            # start off at the first scene after the flood\n",
    "            res = ds[band][0].drop(labels='time')\n",
    "            # loop through the rest of the scenes, if the first scene has invalid pixels, the pixel value\n",
    "            # becomes the value of the second one in the combine first \n",
    "            for i in range(1, l):\n",
    "                res = res.combine_first(ds[band][i].drop(labels='time'))\n",
    "            # save result into the dict\n",
    "            allres[band] = res\n",
    "        else:\n",
    "            # same as above, but this time starting at the most recent scene and working back\n",
    "            res = ds[band][l-1].drop(labels='time')\n",
    "            for i in range(l - 1, 0, -1):\n",
    "                res = res.combine_first(ds[band][i-1].drop(labels='time'))\n",
    "            allres[band] = res\n",
    "            \n",
    "    # combine the results into a new dataset. Force the datatype to float32 just incase it has changed\n",
    "    return xr.Dataset(allres).astype(np.float32)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Running it All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting WESTERNDAVENPORT3 at 2018-02-26 12:24:22.415699\n",
      "Starting 2015-01-13\n",
      "Building BEFORE flood mosaic\n",
      "Loading ls8_nbart_albers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/ipykernel_launcher.py:17: FutureWarning: casting an xarray.Dataset to a boolean will change in xarray v0.11 to only include data variables, not coordinates. Cast the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ls8_nbart_albers\n",
      "Done\n",
      "Building AFTER flood mosaic\n",
      "Loading ls8_nbart_albers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/ipykernel_launcher.py:17: FutureWarning: casting an xarray.Dataset to a boolean will change in xarray v0.11 to only include data variables, not coordinates. Cast the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ls8_nbart_albers\n",
      "Done\n",
      "calculated wetness for ls8\n",
      "thresholded wetness added to array\n",
      "counted\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAADxCAYAAADvEI2QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAFVJJREFUeJzt3X+QXeV93/H3BwF261+AFXuIIIUGOTVJa0MZUIaZ1DY2CLeD/IfTwrSx4mGqdgqpTTxtcdoxLW5nnHQatZ6hJEpQjT2OCSVJrfGoUWRMxtNOhCXHFFtQhg1xzQ7UBEshpB4D0n77xzmrHK13772S7u49R36/Zs7svc89+5yvdHe/+9zv85xzUlVIkvrvjFkHIEmajAlbkgbChC1JA2HClqSBMGFL0kCYsCVpIEzYkrQKkuxM8lySb6zwepJ8MslckkeTXD6uTxO2JK2OTwGbR7x+PbCx3bYBd4/r0IQtSaugqr4MHBqxyxbg09XYB5yT5PxRfZ45zQAlaciue+dr6juHjk6071cffekg8L1O046q2nECh9sAPN15Pt+2PbvSN5iwJan1nUNH+cqeH5lo33XnP/m9qrriFA6XZdpGXivEhC1JrQIWWFirw80DF3aeXwA8M+obrGFLUqsoXqmjE21TsAv4QLtaZBPwQlWtWA4BR9iSdJxpjbCTfA54B7A+yTxwB3AWQFX9CrAbeC8wB3wX+OC4Pk3YktQqiqNTuuR0Vd005vUCbjmRPk3YktSxMHreb6ZM2JLUKuCoCVuShsERtiQNQAGv9Pi2iSZsSWoVZUlEkgah4Gh/87UJW5IWNWc69pcJW5KOCUeXvcRHP5iwJanVTDqasCWp95p12CZsSRqEBUfYktR/jrAlaSCKcLTHV502YUtShyURSRqAIrxc62YdxopM2JLUak6csSQiSYPgpKMkDUBVOFqOsCVpEBYcYUtS/zWTjv1Ni/2NTJLWmJOOkjQgR12HLUn955mOkjQgC64SkaT+ay7+ZMKWpN4rwiuemi5J/VeFJ85I0jDEE2ckaQgKR9iSNBhOOkrSABTxBgaSNAQFvOK1RCRpCOL1sCVpCArPdJSkwejzCLu/f0okaY1VhYU6Y6JtnCSbkzyRZC7J7cu8/iNJHkrytSSPJnnvuD4dYUtSq5l0PPVT05OsA+4C3gPMA/uT7Kqqxzq7/Svg/qq6O8mlwG7golH9mrAl6Zip3dPxSmCuqp4CSHIfsAXoJuwCXt8+fgPwzLhOTdiS1GomHSeuYa9PcqDzfEdV7WgfbwCe7rw2D1y15Pv/NfB7SX4OeA3w7nEHNGFLUscJnOn4fFVdscJry2X9WvL8JuBTVfUfkvwk8JkkP1FVCysd0IQtSa0pnuk4D1zYeX4B31/yuBnYDFBVf5Dk1cB64LmVOl3zVSLjZk4laZYWOGOibYz9wMYkFyc5G7gR2LVkn28B1wAkeSvwauBPRnW6piPsCWdOJWkmquCVhVMfx1bVkSS3AnuAdcDOqjqY5E7gQFXtAj4C/FqS22jKJT9bVUvLJsdZ65LIJDOnkjQTTUlkOoWHqtpNs1Sv2/axzuPHgKtPpM+1TthjZ06TbAO2Aaxj3d/8y8dWvUjSyl7k8PNV9UOn2k+fz3Rc64Q9dua0XRazA+D1Oa+uyjVrEVcvzG3fxCW37Zt1GNIgfbEe+D+n2scJLutbc2udsCeZOf2BZbKWZm16JZHVsNaRTTJzKkkzs9De13HcNgtrmrCr6giwOHP6OM159AfXMoYhmNu+adYhSDM3i9+DZpXIuom2WVjzE2eWmznV8SyNSLP5PfAWYZI0ILMqd0zChC1JLVeJSNKA9HmViAlbklpV4UiPE3Z/I5uQKyokTdNCZaJtFgY/wnZFhaRpsYYtSQNiwpakAXAdtiQNiOuwJWkAquDIFG5gsFpM2JLUYUlEkgbAGrYkDUiZsCVpGPo86djf6vppzLMzpX6q6veZjibsGVg8O9PELfVNOLpwxkTbLFgSmSFPq5f6xxq2JA2A1xKRpKGopo7dVyZsSero8yoRE7YktaqddOwrE7YkdVgSkaSB6PMqkf6O/aUJuJZd01TVJOxJtllwhK1Bcy27pq3Py/ocYU9gzzOPzDoESWukarJtFhxhT+C6H377rEOQtAaKsOAqEUkahh4vErEkIknHTHHSMcnmJE8kmUty+wr7/N0kjyU5mOQ3xvXpCFuSuqYwxE6yDrgLeA8wD+xPsquqHuvssxH4KHB1VR1O8qZx/TrClqSOKY2wrwTmquqpqnoZuA/YsmSffwjcVVWHm+PWc+M6HZuwk+xM8lySb3TazkuyN8mT7ddz2/Yk+WT7EeDRJJd3vmdru/+TSbaOO64krbUCFhYy0QasT3Kgs23rdLUBeLrzfL5t63oL8JYk/zPJviSbx8U3yQj7U8DSjm4HHqyqjcCD7XOA64GN7bYNuBuaBA/cAVxF85fnjsUkL0m9UUBlsg2er6orOtuOTk/LDcGXFlvOpMmV7wBuAn49yTmjwhubsKvqy8ChJc1bgHvbx/cC7+u0f7oa+4BzkpwPXAfsrapD7fB/L9//R0CSZm5K67DngQs7zy8Anllmn89X1StV9cfAEzQJfEUnW8N+c1U9C9B+XSyWr/QxYJKPBwAk2bb4EeMVXjrJ8CTpJNWE22j7gY1JLk5yNnAjsGvJPv8NeCdAkvU0JZKnRnU67UnHlT4GTPLxoGms2rH4EeMsXjXV4CRptMkmHMdNOlbVEeBWYA/wOHB/VR1McmeSG9rd9gDfSfIY8BDwz6rqO6P6PdmE/e221EH7dXF2c6WPAZN8PDhteEEiacCmM8KmqnZX1Vuq6ker6t+1bR+rql3t46qqn6+qS6vqr1fVfeP6PNmEvQtYXOmxFfh8p/0D7WqRTcALbclkD3BtknPbycZr27bTkhckkgaqoBYy0TYLkyzr+xzwB8CPJZlPcjPwCeA9SZ6kWRj+iXb33TQ1mDng14B/AlBVh4CP09R19gN3tm2nPUfb0tBkwm3tjT3TsapuWuGla5bZt4BbVuhnJ7DzhKIbqD3PPOIFo6Sh6vHFRDzTcRWslKxHjbYdiUs9MaUa9mrwWiKr7JLb9jG3fRN/9Pd+het+ePR+kmZs8cSZnjJhr7K57Zu45LZ9/Cj/mEswKUt91+eb8FoSWWWLI+fuCNryh9RjC5lsmwFH2GtoMVEvlkksg0j9E0fYguNH2SZrqYcmnXB00vEHg4la6rM46ShJg9HjkogJW5K6FmYdwMpM2JK0yHXYkjQcfV4lYsKWpK4eJ2yX9UnSQDjClqQOSyKSNATFzE47n4QJW5K6HGFL0jBYEpGkoTBhS9JAmLAlqf9SlkQkaThcJSJJw+AIW5KGwoQtSQNgDVuSBsSELUnDkB7fwMCr9UlaE3PbN806hMFzhC1pTQzmBtSWRCRpAJx0lKQBMWFL0kCYsCWp/8LAV4kkuTDJQ0keT3IwyYfa9vOS7E3yZPv13LY9ST6ZZC7Jo0ku7/S1td3/ySRbV++fJUknof7iAlDjtnGSbE7yRJsLbx+x3/uTVJIrxvU5ybK+I8BHquqtwCbgliSXArcDD1bVRuDB9jnA9cDGdtsG3N0GdR5wB3AVcCVwx2KSl6TeqAm3EZKsA+6iyYeXAje1eXPpfq8D/inw8CShjU3YVfVsVf1h+/hF4HFgA7AFuLfd7V7gfe3jLcCnq7EPOCfJ+cB1wN6qOlRVh4G9wOZJgpSkNTOFhE0zKJ2rqqeq6mXgPprcuNTHgV8CvjdJaCd04kySi4DLaP4avLmqnoUmqQNvanfbADzd+bb5tm2l9qXH2JbkQJIDr/DSiYQnSafsBEoi6xdzVbtt63QzNt8luQy4sKq+MGlsE086Jnkt8FvAh6vqz5IVrxm73As1ov34hqodwA6A1+e8Hs/XSjotTZ51nq+qlerOI/NdkjOA7cDPnkhoE42wk5xFk6w/W1W/3TZ/uy110H59rm2fBy7sfPsFwDMj2iWpH6pZJTLJNsa4fPc64CeA30/yTZr5wV3jJh4nWSUS4B7g8ar65c5Lu4DFlR5bgc932j/QrhbZBLzQlkz2ANcmObedbLy2bZOk/phODXs/sDHJxUnOBm6kyY3NIapeqKr1VXVRVV0E7ANuqKoDozqdpCRyNfAzwNeTPNK2/QLwCeD+JDcD3wJ+un1tN/BeYA74LvDBNsBDST7e/kMA7qyqQxMcX5LWzDROTa+qI0lupRmUrgN2VtXBJHcCB6pq1+geljc2YVfV/2D5egzANcvsX8AtK/S1E9h5IgFK0pqa0sxZVe2mGcB22z62wr7vmKRPz3SUpEWTlTtmxoQtSa3Q76v1DeYGBl78XNJamNap6athMCPswVz8XNKw9XiEPZiELUlrwoQtSQPgHWckaUBM2JI0DIO+gYHUNbd907EVO67c0emoz6tETNg6IYurdea2b3Lljk4/k15HxGV9GopuojZx67RjDVunK5O1Tid9P9PRhC1JHVnob8Y2YUvSIi/+JEnD0eeSiKtEpFXm8seB6fEqERO2tMouuW2fSXtA+rwO25KIJHVZEpE0i1G2I/sTNL27pq8KE7a0Bma1Xt118idmcR22JRHpB5zJcyCqvzURE7YkdfR5WZ8JW5IWeeKMJA1Hn6+HbcKWpA4TtiQNQeGkoyQNhZOOkjQUJmxJ6j9vYCBJQ1HlDQwkjbZ4zQ/PhuyB/ubrfifsFzn851+sB56YdRxjrAeen3UQIxjfqVmb+D78AADfPLnv9v+w8Vem0YklkZP3RFVdMesgRklyoM8xGt+p6Xt80P8Y+x7fcQqwJCJJA9HffO3lVSWpa1qXV02yOckTSeaS3L7M6z+f5LEkjyZ5MMnYkk7fE/aOWQcwgb7HaHynpu/xQf9j7Ht8x8lCTbSN7CNZB9wFXA9cCtyU5NIlu30NuKKq/gbwAPBL42LrdcKuqt6/0X2P0fhOTd/jg/7H2Pf4jjPpDXjHj7CvBOaq6qmqehm4D9hy3KGqHqqq77ZP9wEXjOvUGrYktZoTZyYuYq9PcqDzfEfnj9MG4OnOa/PAVSP6uhn47+MOaMKWpK7Jr9b3/IjVL1mmbdm/BEn+AXAF8LfGHbC3JZFxBftVPO7OJM8l+Uan7bwke5M82X49t21Pkk+2MT6a5PLO92xt938yydYpxndhkoeSPJ7kYJIP9SnGJK9O8pUk/6uN79+07Rcnebg91m8mObttf1X7fK59/aJOXx9t259Ict004uv0vS7J15J8oafxfTPJ15M8sjiK68t73PZ7TpIHkvzv9mfxJ/sU36lI1UTbGPPAhZ3nFwDPfN+xkncD/xK4oapeGtdpLxN2JivYr5ZPAZuXtN0OPFhVG4EH2+e08W1st23A3dD8YgF30HwEuhK4Y/GHdwqOAB+pqrcCm4Bb2v+bvsT4EvCuqnob8HZgc5JNwC8C29v4DtN8BKT9eriqLgG2t/vR/ptuBH6c5v34z+3PxbR8CHi887xv8QG8s6re3hnF9eU9BvhPwO9W1V8D3kbzf9mn+E7O9GrY+4GN7UDgbJqflV3dHZJcBvwqTbJ+bpLwepmwmaBgv1qq6svAoSXNW4B728f3Au/rtH+6GvuAc5KcD1wH7K2qQ1V1GNjL9/8RONn4nq2qP2wfv0jzi7KhLzG2x/nz9ulZ7VbAu2hmwpeLbzHuB4BrkqRtv6+qXqqqPwbmaH4uTlmSC4C/Dfx6+zx9im+EXrzHSV4P/BRwD0BVvVxVf9qX+E7NZCtExq0SqaojwK3AHprf0fur6mCSO5Pc0O7274HXAv+1/SS1a4XujulrDftEC/ar7c1V9Sw0CTPJm9r25eLcMKJ9qtqP55cBD/cpxnak+VXgEppPSn8E/Gn7Q7z0WMfiqKojSV4A3ti2dy+sMc3/w/8I/HPgde3zN/YsPmj+yP1ekgJ+tZ3M6st7/FeBPwH+S5K30bzXH+pRfKdmSjcwqKrdwO4lbR/rPH73ifbZ1xH2xAX7GVspzlWPP8lrgd8CPlxVfzZq1xViWbUYq+poVb2dpm53JfDWEcda0/iS/B3guar6ard5xLFm9R5fXVWX05QTbknyUyP2XesYzwQuB+6uqsuA/8dflD+WM7PfkxNWzS3CJtlmoa8Je6KC/Rr6dvsRjvbrYr1ppThXNf4kZ9Ek689W1W/3MUaA9mPy79PU2s9JsviJrnusY3G0r7+BpiS1WvFdDdyQ5Js0pbZ30Yy4+xIfAFX1TPv1OeB3aP7w9eU9ngfmq+rh9vkDNAm8L/GdmqrJthnoa8IeW7BfY7uAxRnsrcDnO+0faGfBNwEvtB8J9wDXJjm3nUS5tm07ZW399B7g8ar65b7FmOSHkpzTPv5LwLtpangPAe9fIb7FuN8PfKmqqm2/Mc0qjYtpJqy+cqrxVdVHq+qCqrqI5ufqS1X19/sSH0CS1yR53eJjmvfmG/TkPa6q/ws8neTH2qZrgMf6Et8pm86k46roZQ27rRUuFuzXATur6uBaHDvJ54B30CyKn6eZxf4EcH+Sm4FvAT/d7r4beC/NhNN3gQ+28R9K8nGaPzwAd1bV0onMk3U18DPA15M80rb9Qo9iPB+4t61jn0Ez2fKFJI8B9yX5tzSn5N7T7n8P8JkkczQj1xvb+A4muZ8mERwBbqmqo1OIbyX/okfxvRn4neZvM2cCv1FVv5tkP/14jwF+DvhsO6B6qj3mGT2K76Rlob+3TU/1+A7BkrSWXv+aDbXpx//RRPvu3X/HV0ecOLMqejnClqRZCBOdFDMzJmxJ6jJhS9JAmLAlaQCKE7n405ozYUtSR59XiZiwJemY2Z0UMwkTtiQtKkzYkjQY/a2ImLAlqct12JI0FCZsSRqAKjja35qICVuSuhxhS9JAmLAlaQAKGHO/xlkyYUvSMQVlDVuS+q9w0lGSBsMatiQNhAlbkobAiz9J0jAU4OVVJWkgHGFL0hB4arokDUNBuQ5bkgbCMx0laSCsYUvSAFS5SkSSBsMRtiQNQVFHj846iBWZsCVpkZdXlaQB6fGyvjNmHYAk9UUBtVATbeMk2ZzkiSRzSW5f5vVXJfnN9vWHk1w0rk8TtiQtqvYGBpNsIyRZB9wFXA9cCtyU5NIlu90MHK6qS4DtwC+OC8+ELUkddfToRNsYVwJzVfVUVb0M3AdsWbLPFuDe9vEDwDVJMqpTa9iS1HqRw3u+WA+sn3D3Vyc50Hm+o6p2tI83AE93XpsHrlry/cf2qaojSV4A3gg8v9IBTdiS1KqqzVPqarmR8tLC9yT7HMeSiCRN3zxwYef5BcAzK+2T5EzgDcChUZ2asCVp+vYDG5NcnORs4EZg15J9dgFb28fvB75UNfo0S0sikjRlbU36VmAPsA7YWVUHk9wJHKiqXcA9wGeSzNGMrG8c12/GJHRJUk9YEpGkgTBhS9JAmLAlaSBM2JI0ECZsSRoIE7YkDYQJW5IG4v8D3UkgZYE6WlcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0523b0fba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote Tasselled Cap Wetness netCDF for westerndavenport3_2015\n",
      "Creating output file that is 6655P x 2928L.\n",
      "Processing input file westerndavenport3_2015.nc.\n",
      "Using internal nodata values (e.g. 9.96921e+36) for image westerndavenport3_2015.nc.\n",
      "Copying nodata values from source westerndavenport3_2015.nc to destination westerndavenport3_2015.tif.\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "GeoTiff has been saved\n",
      "Finished 2015-01-13\n",
      "Finished WESTERNDAVENPORT3 at 0:05:10.024179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "study_areas = ['westerndavenport3'] #'tennantcreek', 'westerndavenport1','westerndavenport2','titree1','titree2','alicesprings',\n",
    "flood_dates = ['2015-01-13']  #,'2010-03-01','2000-04-22'\n",
    "starttime = datetime.datetime.now()\n",
    "\n",
    "for study_area in study_areas:\n",
    "    print('Starting ' + study_area.upper() + ' at ' + str(starttime))\n",
    "    for flood_date in flood_dates:\n",
    "        print('Starting ' + flood_date)\n",
    "        for flood_status in [True, False]:\n",
    "            if flood_status:\n",
    "                print('Building BEFORE flood mosaic')\n",
    "            else:\n",
    "                print('Building AFTER flood mosaic')\n",
    "                \n",
    "            ds = getData(study_area, flood_date, 60, before_flood = flood_status)\n",
    "            attribs = ds.attrs\n",
    "            ds = maskCloudAndTerrain(ds, cloud = True, terrain = True)\n",
    "            ds = mosaic(ds, before_flood = flood_status)\n",
    "            ds.attrs = attribs\n",
    "            f = open(str(flood_status) + 'temp.pkl', 'wb')\n",
    "            pkl = pickle.dump(ds, f)\n",
    "            f.close()\n",
    "            del ds\n",
    "            gc.collect()\n",
    "            print('Done')\n",
    "        res = []\n",
    "        f = open('Truetemp.pkl','rb')\n",
    "        res.append(pickle.load(f))\n",
    "        f.close()\n",
    "        f = open('Falsetemp.pkl','rb')\n",
    "        res.append(pickle.load(f))\n",
    "        f.close()\n",
    "        ds = xr.concat(res,pd.Index([0,1])).rename({'concat_dim':'time'})\n",
    "        ds = calcWetnessProportion(ds)\n",
    "        createGeoTiff(ds, study_area, flood_date[0:4])\n",
    "        del ds\n",
    "        del res\n",
    "        gc.collect()\n",
    "        print('Finished ' + flood_date)\n",
    "    print('Finished ' + study_area.upper() + ' at ' + str(datetime.datetime.now() - starttime) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "906px",
    "left": "0px",
    "right": "1466px",
    "top": "111px",
    "width": "268px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
